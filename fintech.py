# -*- coding: utf-8 -*-
"""fintech_CS3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/140yRn9exEsd-PEeKQK-fCMsI4ly9NzPS

##Importing Libraries and dataset
"""

import numpy as np;
import pandas as pd;
import matplotlib.pyplot as plt;
import seaborn as sns;
from dateutil import parser;
dataset=pd.read_csv("/content/drive/My Drive/Dataset/appdata10.csv");
print("Dataset Extracted");

"""##Dataset Description and Extracting"""

dataset.head()

dataset.describe()

#Data Cleaning
dataset.hour=dataset.hour.str.slice(1,3).astype(int);
num_dataset=dataset.drop(columns=['user','first_open','screen_list','enrolled','enrolled_date'],axis=1);
num_dataset.head()

"""##Data Visualization"""

sns.kdeplot(data=dataset.age,shade=True);

sns.scatterplot(x=dataset.age,y=dataset.numscreens,hue=dataset.enrolled);

plt.figure(figsize=(20,10));
plt.suptitle("Histogram of Numerical Values",fontsize=20);
for i in range(0,num_dataset.shape[1]):
  plt.subplot(2,4,i+1);
  f=plt.gca();
  f.set_title(num_dataset.columns[i]);
  vals=num_dataset.iloc[:,i].nunique();
  plt.hist(num_dataset.iloc[:,i],bins=vals);
plt.tight_layout(rect=[0,0.03,1,0.95]);
#plt.savefig('Histogram.jpg')

"""###Correlation with Target Variable"""

num_dataset.corrwith(dataset.enrolled).plot.bar(figsize=(12,8),fontsize=15,title="Correlation with Target Variable",rot=-45);

plt.figure(figsize=(10,8));
corr=num_dataset.corr();
mask=np.zeros_like(corr,dtype=np.bool);
mask[np.triu_indices_from(mask)]=True;
plt.title("Correlation Matrix");
sns.heatmap(corr,mask=mask,annot=True,center=0);
plt.xticks(rotation=-45);

"""##Feature Engineering

###Analysing Date Column
"""

date_dataset=dataset.copy()
date_dataset.dropna(inplace=True);
date_dataset.reset_index(inplace=True);
dataset.drop(columns=['first_open','enrolled_date'],axis=1,inplace=True);
date_dataset.drop(columns=dataset.columns,axis=1,inplace=True);
print(date_dataset.head());
print(date_dataset.dtypes);

date_dataset.first_open   =[parser.parse(row_data) for row_data in date_dataset.first_open];
date_dataset.enrolled_date=[parser.parse(row_data) for row_data in date_dataset.enrolled_date];
date_dataset['diff']=(date_dataset.enrolled_date-date_dataset.first_open).astype('timedelta64[h]');
print(date_dataset.head());
print(date_dataset.dtypes);

plt.figure(figsize=(20,5));
plt.suptitle("Histogram of Difference in time between enrolled time and first open(only premium users)",fontsize=15);
plt.subplot(1,3,1);
plt.hist(date_dataset['diff']);
plt.title("All Premium User");
plt.subplot(1,3,2);
plt.hist(date_dataset['diff'],range=(0,720));
plt.title("Premium User who enrolled within one month");
plt.subplot(1,3,3);
plt.hist(date_dataset['diff'],range=(0,48));
plt.title("Premium User who enrolled within two day");
plt.show();

premium_user=date_dataset.shape[0];
instant_user=len([i for i in range(0,premium_user) if date_dataset['diff'][i]==0]);
lazy_user=len([i for i in range(0,premium_user) if date_dataset['diff'][i]>48]);
print(str(round(100*instant_user/premium_user,1)) + "% Percentage of Premium User enrolled within one hour");
print(str(round(100*lazy_user/premium_user,1)) + "% Percentage of Premium User enrolled after 2 days");
plt.hist(date_dataset['diff'],range=(1,48));
plt.title("Premium User who Enrolled in atleast one and within 2 days");

"""###Analysing Screen"""

top_screen=pd.read_csv("/content/drive/My Drive/Dataset/top_screens.csv").top_screens.values;

#Mapping Screen to Fields
dataset.screen_list=dataset.screen_list.astype(str)+",";
for sc in top_screen:
  dataset[sc]=dataset.screen_list.str.contains(sc).astype(int);
  dataset.screen_list=dataset.screen_list.str.replace(sc+",","");
dataset['Other']=dataset.screen_list.str.count(",");
dataset.drop(columns=['screen_list'],axis=1,inplace=True);

#Funnels
savings_screens = ["Saving1", "Saving2", "Saving2Amount", "Saving4", "Saving5", "Saving6","Saving7", "Saving8", "Saving9", "Saving10"];
dataset['SavingCount']=dataset[savings_screens].sum(axis=1);
dataset.drop(columns=savings_screens,axis=1,inplace=True);
credit_screens = ["Credit1", "Credit2", "Credit3", "Credit3Container", "Credit3Dashboard"];
dataset['CreditCount']=dataset[credit_screens].sum(axis=1);
dataset.drop(columns=credit_screens,axis=1,inplace=True);
cc_screens = ["CC1", "CC1Category", "CC3"];
dataset['CCCount']=dataset[cc_screens].sum(axis=1);
dataset.drop(columns=cc_screens,axis=1,inplace=True);
loan_screens = ["Loan", "Loan2", "Loan3", "Loan4"];
dataset['LoanCount']=dataset[loan_screens].sum(axis=1);
dataset.drop(columns=loan_screens,axis=1,inplace=True);

"""##Final Dataset Preview and Extraction to new dataset"""

dataset.head()

dataset.shape

dataset.describe()

"""## Data Preprocessing for Model Building"""

response=dataset['enrolled'];
dataset.drop(columns=['enrolled','user'],axis=1,inplace=True);

from sklearn.model_selection import train_test_split;
X_train,X_test,y_train,y_test=train_test_split(dataset,response,test_size=0.2)

from sklearn.preprocessing import StandardScaler;
sc=StandardScaler();
sc_X_train=pd.DataFrame(sc.fit_transform(X_train));
sc_X_test =pd.DataFrame(sc.transform(X_test));
sc_X_train.columns=X_train.columns;
sc_X_test.columns=X_test.columns;

"""##Model Building"""

#Confusion Matrix for Binary Classification Problem
from sklearn.metrics import confusion_matrix;
def heatmap_confusion_matrix(y_true,y_pred):
  cm=confusion_matrix(y_true,y_pred);
  group_names = ['True Neg','False Pos','False Neg','True Pos'];
  group_counts = ['{0:0.0f}'.format(value) for value in cm.flatten()];
  group_percent = ['{0:.2%}'.format(value) for value in cm.flatten()/np.sum(cm)];
  labels = [f'{v1}\n{v2}\n{v3}' for v1, v2, v3 in zip(group_names,group_counts,group_percent)];
  labels = np.asarray(labels).reshape(2,2);
  plt.figure(figsize=(6,4));
  sns.set(font_scale=1.2);
  sns.heatmap(cm,annot=labels,fmt='',cmap='Blues');
  plt.xlabel("Predicted Value");
  plt.ylabel("Actual Value");
  plt.show();

#Metrics for Classification Problem
from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score;
def classification_metrics(y_true,y_pred):
  print("Accuracy Score  : ",round(accuracy_score(y_true,y_pred),3));
  print("Precision Score : ",round(precision_score(y_true,y_pred),3));
  print("Recall Score    : ",round(recall_score(y_true,y_pred),3));
  print("F1 Score        : ",round(f1_score(y_true,y_pred),3));

#Apply K-fold Cross Validation for Classification Problem
from sklearn.model_selection import cross_val_score;
def k_fold_cross_validation(model,train_data,train_response,number):
  accuracies=cross_val_score(estimator=model,X=train_data,y=train_response,cv=number);
  print("Mean Accuracy of Model : ",round(accuracies.mean(),3))
  print("Standard Deviation is  : ",round(accuracies.std(),3));

"""### Training on Logistic Regression Model"""

from sklearn.linear_model import LogisticRegression;
log_classifier=LogisticRegression(penalty='l1',solver='liblinear');
log_classifier.fit(sc_X_train,y_train);
log_y_pred=log_classifier.predict(sc_X_test);
print("Logistic Regression Model");
heatmap_confusion_matrix(y_test,log_y_pred);
classification_metrics(y_test,log_y_pred);
k_fold_cross_validation(log_classifier,X_train,y_train,10);

"""#### Model Tuning using Grid Search Cv"""

parameter={'C':[0.01,0.1,1,10,100],
           'penalty':['l1','l2']};
from sklearn.model_selection import GridSearchCV;
grid=GridSearchCV(estimator=log_classifier,param_grid=parameter,scoring='accuracy',n_jobs=-1);
grid.fit(X_train,y_train);
print("Best Parameter : ",grid.best_params_);
print("Best Accuracy  : ",round(grid.best_score_,3));

parameter={'C':[2,5,10,20,50],
           'penalty':['l1','l2']};
from sklearn.model_selection import GridSearchCV;
grid_1=GridSearchCV(estimator=log_classifier,param_grid=parameter,scoring='accuracy',n_jobs=-1);
grid_1.fit(X_train,y_train);
print("Best Parameter : ",grid_1.best_params_);
print("Best Accuracy  : ",round(grid_1.best_score_,3));

y_grid=grid_1.predict(X_test);
print("Logistic Regression after Hyperparameter Tuning");
heatmap_confusion_matrix(y_test,y_grid);
classification_metrics(y_test,log_y_pred);
k_fold_cross_validation(log_classifier,X_train,y_train,10);

"""### Training on XGboost Model"""

from xgboost import XGBClassifier;
boost_classifier=XGBClassifier();
boost_classifier.fit(X_train,y_train);
boost_y_pred=boost_classifier.predict(X_test);
print("XG Boost Model Performance");
heatmap_confusion_matrix(y_test,boost_y_pred);
classification_metrics(y_test,boost_y_pred);
k_fold_cross_validation(boost_classifier,X_train,y_train,10);
